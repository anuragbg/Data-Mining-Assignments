---
title: "Exercise 9"
output: statsr:::statswithr_lab
author: Jacob John
---

<div id="instructions">
Complete all **Exercises**, and submit answers to **VtopBeta**
</div>


##Introduction

Locality Sensitive Hashing is a method of performing probabilistic dimension reduction of high-dimensional data. 

##Dataset

A minhash function converts tokenized text into a set of hash integers, then selects the minimum value. This is the equivalent of randomly selecting a token. 

The function then does the same thing repeatedly with different hashing functions, in effect selecting n random shingles. The additional hashing functions come from a bitwise XOR with random integers. That is why the *minhash_generator()* accepts a seed, so that we can re-create the same minhash function again. In other words, a minhash function converts a set of tokens of any length into n randomly selected and hashed tokens.

```{r dataset, message = FALSE}
library(textreuse)
minhash <- minhash_generator(n = 240, seed = 3552)
head(minhash(c("turn tokens into", "tokens into hashes", "into hashes fast")))
```

##Minhash

Now when we load our text corpus, we will tokenize our texts as usual, but we will use our generated *minhash()* function to compute the hashes. We specify that we want to create a minhash signature by passing our minhash function to the *minhash_func =* parameter.

```{r minhash, message = FALSE}
dir <- system.file("extdata/ats", package = "textreuse")
corpus <- TextReuseCorpus(dir = dir, tokenizer = tokenize_ngrams, n = 5,
                          minhash_func = minhash, keep_tokens = TRUE,
                          progress = FALSE)

# verify minhash exists in the corpus
corpus
head(minhashes(corpus[[1]])) # minhash signature of the document
length(minhashes(corpus[[1]]))
```


##Locality Sensitive Hashing

Now all our documents are represented by *n = 240* randomly selected and hashed shingles. Comparing those shingles should be the equivalent of finding the Jaccard similarity of the two documents. However, we still have the problem of pairwise comparison.

The locality-sensitive hashing algorithm, provided in this package by the *lsh()* function, solves this problem. LSH breaks the minhashes into a series of bands comprised of rows. For example, 200 minhashes might broken into 50 bands of 4 rows each. Each band is hashed to a bucket. If two documents have the exact same minhashes in a band, they will be hashed to the same bucket, and so will be considered candidate pairs. Each pair of documents has as many chances to be considered a candidate as their are bands, and the fewer rows there are in each band, the more likely it is that each document will match another. We can calculate the likely threshold based on the number of minhashes and bands that we are using.

```{r lsh, message = FALSE}
lsh_threshold(h = 200, b = 50)
lsh_threshold(h = 240, b = 80)
lsh_probability(h = 240, b = 80, s = 0.25) #similarity of over 0.25
lsh_probability(h = 240, b =  80, s = 0.75) # similarity of over 0.75
```

These numbers seem reasonable for our purposes, so we will set the number of minhashes at 240 and the number of bands at 80.

Now we can use the *lsh()* function to calculate the locality-sensitive hashes for our documents.

```{r lsh2, message = FALSE}
buckets <- lsh(corpus, bands = 80, progress = FALSE)
buckets
```

We can extract the potential matches from the cache using lsh_query() or lsh_candidates(). The first function returns matches for only one document, specified by its ID; the second functions returns all potential pairs of matches.

```{r lsh3, message = FALSE}
baxter_matches <- lsh_query(buckets, "calltounconv00baxt") # ID of the corpus
baxter_matches
candidates <- lsh_candidates(buckets)
candidates
```

##Inference

Notice that LSH has identified the same three pairs of documents as potential matches that we found with pairwise comparisons, but did so much faster. But we do not have similarity scores; we only know that these documents are likely to have Jaccard similarity scores above the 0.232 threshold.

Now we can use *lsh_compare()* to apply a similarity function to the candidate pairs of documents. Note that we only have to do 3 comparisons for all the candidates, instead of 28 pairs when comparing all 8 documents in the corpus pairwise.

```{r plot, message = FALSE}
lsh_compare(candidates, corpus, jaccard_similarity, progress = FALSE)
```

Note that these results require much less computation.
